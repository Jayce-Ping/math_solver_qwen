# Inference configuration for math_solver_qwen
max_tokens: 4096
temperature: 0.7
top_p: 0.9
batch_size: 4
use_tool_calls: true

# Model loading configuration
model_path: "models/vision_r1_3b"
model_name: "vision_r1_3b"
devices: "-1"  # Use all available devices
tensor_parallel_size: 4
dtype: "float16"